{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "potential-character",
   "metadata": {},
   "source": [
    "# Building PyTorch AlexNet model from scratch\n",
    "\n",
    "## By Kelvin Hong, 5 March 2021\n",
    "\n",
    "This notebook will show you how to create an AlexNet model using this [AlexNet Paper](https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf). This model will not be perfectly same as described in the paper, as this Convolutional Neural Network (CNN) is to perform classification on a small dataset. \n",
    "\n",
    "The originial AlexNet with an input of size $(3, 227, 227)$ will go through 5 convolutional layers:\n",
    "$$(3, 227, 227)\\xrightarrow{\\text{Conv}} (96, 55, 55)\\xrightarrow{\\text{Conv}} (256, 27, 27)\\xrightarrow{\\text{Conv}} (384, 13, 13)\\xrightarrow{\\text{Conv}} (384, 13, 13)\\xrightarrow{\\text{Conv}} (128, 13, 13).$$\n",
    "We take the output with size $128\\times 13^2$, perform max-pooling to get $128\\times 6^2$ then flatten it to get $4608$ parameters. Continued with 3 fully connected layers, the dimensions become\n",
    "$$4608\\xrightarrow{\\text{FC}} 4096\\xrightarrow{\\text{FC}} 4096\\xrightarrow{\\text{FC}} 1000\\text{ classes.}$$\n",
    "\n",
    "Our task will be using AlexNet model to classify 1125 images from the [weather](https://data.mendeley.com/datasets/4drtyfjtfy/1) datasets into 4 classes (cloudy, rain, sunrise, shine). Thus we will be using a simplified version of AlexNet, with convolutional layers: \n",
    "$$(3, 224, 224)\\xrightarrow{\\text{Conv}} (16, 55, 55)\\xrightarrow{\\text{Conv}} (32, 27, 27)\\xrightarrow{\\text{Conv}} (64, 13, 13)\\xrightarrow{\\text{Conv}} (128, 13, 13)\\xrightarrow{\\text{Conv}} (128, 13, 13),$$\n",
    "and fully connected layers\n",
    "$$4608\\xrightarrow{\\text{FC}} 2048\\xrightarrow{\\text{FC}} 1024\\xrightarrow{\\text{FC}} 4\\text{ classes.}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "hired-arizona",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First import all necessary modules.\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import torchsummary\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import time\n",
    "from PIL import Image\n",
    "from urllib.request import urlopen\n",
    "import argparse\n",
    "import re\n",
    "\n",
    "# A handy function for convert seconds to HH-MM-SS format.\n",
    "def convert_sec(t):\n",
    "    h = int(t//3600)\n",
    "    m = int((t%3600)//60)\n",
    "    s = int(t%60)\n",
    "    if h!=0:\n",
    "        t_str = f\"{h}h {m}m {s}s\"\n",
    "    elif m!=0:\n",
    "        t_str = f\"{m}m {s}s\"\n",
    "    else:\n",
    "        t_str = f\"{s}s\"\n",
    "    return t_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "advisory-incident",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "searching-dayton",
   "metadata": {},
   "source": [
    "## Prepare data\n",
    "\n",
    "Before we begin, create a `Tutorial` folder in current directory then save the zip file from [weather](https://data.mendeley.com/datasets/4drtyfjtfy/1) into `Tutorial` folder. After extracting the zip file we will see a `dataset2` folder which contains everything we want. This folder only contains thousands of images related to weather. They will have image names like `cloudy1.jpg` or `rain157.jpg`. We will make use of this to create list of images and list of labels (ground truth) in Python. \n",
    "\n",
    "The location of an image will be like `./Tutorial/dataset2/cloudy1.jpg`. Save this notebook in the home directory, hence `./Tutorial_PyTorch_Build_AlexNet_Model.ipynb`.\n",
    "\n",
    "A basic structure:\n",
    "```\n",
    "./\n",
    "├─── Tutorial/\n",
    "|    ├──── dataset2/\n",
    "|          ├──── [images]\n",
    "├─── Tutorial_PyTorch_Build_AlexNet_Model.ipynb\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "anonymous-collector",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "images_list = os.listdir('./Tutorial/dataset2')\n",
    "# Correspond labels into indices\n",
    "labels = {'cloudy': 0, 'rain': 1, 'shine': 2, 'sunrise': 3}\n",
    "# Below labels_list contains labels 0, 1, 2, 3 corresponding to the images list.\n",
    "labels_list = [labels[image_name[:re.search(r\"\\d\", image_name).start()]] for image_name in images_list]\n",
    "# Split datasets into train and test with ratios 0.7, 0.3.\n",
    "n = len(labels_list)\n",
    "train_ratio = 0.7\n",
    "train_n = int(train_ratio*n)\n",
    "train_images = images_list[:train_n]\n",
    "test_images = images_list[train_n:]\n",
    "train_labels = labels_list[:train_n]\n",
    "test_labels = labels_list[train_n:]\n",
    "test_n = n-train_n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supposed-implement",
   "metadata": {},
   "source": [
    "## Using DataLoader from torch\n",
    "\n",
    "We will use `torch.utils.data.DataLoader` to load the data, by inherit the DataLoader. The benefit is it can generate minibatch in one-line code, as well as provide randomness. \n",
    "\n",
    "Since I only use four lists (train, test images and train, test labels), my method for using DataLoader can be generalized to other datasets as well. \n",
    "\n",
    "### Define custom Dataset class\n",
    "First, we need to define a custom `DataSet` object. From the [official documentation](https://pytorch.org/docs/stable/data.html), the dataset type is either Map-style or Iterable-style. We will be using Map-style dataset, hence we have to look into the `torch.utils.data.Dataset` class, then override the `__len__()` and `__getitem__()` protocols.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "certified-instrumentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom Dataset for later use of DataLoader\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    # Take in a list of image paths, and a list of numeric labels (integers). Transform will be explained below.\n",
    "    def __init__(self, images_list, labels_list, transform):\n",
    "        self.images = [os.path.join('./Tutorial/dataset2/',path) for path in images_list]\n",
    "        self.labels = labels_list\n",
    "        self.transform = transform\n",
    "    \n",
    "    # Define the length of dataset. Syntax len(dataset)\n",
    "    def __len__(self):\n",
    "        images_n = len(self.images)\n",
    "        labels_n = len(self.labels)\n",
    "        if images_n != labels_n:\n",
    "            print(f\"SizeError: number of images [{images_n}] and number of labels [{labels_n}] should be same.\")\n",
    "            return None\n",
    "        return images_n\n",
    "    # Define the get function. Syntax dataset[index]\n",
    "    def __getitem__(self, index):\n",
    "        # Convert to RGB to make sure input channel = 3\n",
    "        img = Image.open(self.images[index]).convert('RGB')\n",
    "        # First we applies transform on image, then using to(device) method, \n",
    "        # to align with the input device of model.\n",
    "        sample = {'image': self.transform(img).to(device), 'label': self.labels[index]}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "injured-horizontal",
   "metadata": {},
   "source": [
    "### Transform, DataLoader\n",
    "\n",
    "The main purpose of `transform` is to first preprocess and perform data augmentations to the training dataset (as well as test dataset). Base on the model architecture, we first resize image to `(240,240)`, then apply Random Crop to produce `(227,277)` image to make the training more robust. The `ToTensor()` method will transform PIL image into tensor format, then finally we `Normalize()` each number in the tensor with the given mean and standard deviation on the three color channels.\n",
    "\n",
    "After using `transform` to prepare the dataset, we dump the variables into DataLoader, with batch size equals to 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "humanitarian-baseline",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "        transforms.Resize((240,240)),\n",
    "        transforms.RandomCrop(227),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406),(0.229,0.224,0.225))\n",
    "    ])\n",
    "\n",
    "# For more customization, one can define transforms for train and test datasets separately. \n",
    "train_dataset = CustomDataset(train_images, train_labels, transform=transform)\n",
    "test_dataset = CustomDataset(test_images, test_labels, transform=transform)\n",
    "\n",
    "# Shuffle the trainloader to provide randomness\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "initial-daniel",
   "metadata": {},
   "source": [
    "## Define AlexNet model\n",
    "\n",
    "To define a whole-new model, we have to define how to forward in the network, hence we need to inherit the class `torch.nn.Module`. \n",
    "\n",
    "*Noticed we have used `Dropout` in the model, it is to drop some proportion of weights, and this can weaken the correlation between adjacent nodes, making them independent and robust.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "sized-yesterday",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class Alex_Custom(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \"\"\" \n",
    "        In convolution layers below, in_channels and out_channels are\n",
    "        respectively: Current filter & Future filter. \n",
    "        (The first convolution layer is color channels & Future filter)\n",
    "        \n",
    "        We change how the filters (channels) behave between each Convolutional network\n",
    "        Because the network is for training a relatively small dataset.\n",
    "        The overall model architecture is still the same.\n",
    "        \n",
    "        Check the paper here: https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf\n",
    "        \n",
    "        \"\"\" \n",
    "        self.conv1 = torch.nn.Conv2d(3, 16, 11, stride=4)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.batchnorm1 = torch.nn.BatchNorm2d(16)\n",
    "        self.conv2 = torch.nn.Conv2d(16, 32, 5, padding=2)\n",
    "        self.batchnorm2 = torch.nn.BatchNorm2d(32)\n",
    "        self.maxp = torch.nn.MaxPool2d(2,2)\n",
    "        self.conv3 = torch.nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv4 = torch.nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.conv5 = torch.nn.Conv2d(128, 128, 3, padding=1)\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.linear1 = torch.nn.Linear(6*6*128, 2048)\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "        self.linear2 = torch.nn.Linear(2048,1024)\n",
    "        self.linear3 = torch.nn.Linear(1024, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.batchnorm1(self.conv1(x))\n",
    "        x = self.maxp(self.batchnorm2(self.relu(self.conv2(x))))\n",
    "        x = self.maxp(self.relu(self.conv3(x)))\n",
    "        x = self.conv4(x)\n",
    "        x = self.maxp(self.relu(self.conv5(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hybrid-peoples",
   "metadata": {},
   "source": [
    "### Custom weights initialization\n",
    "\n",
    "We can control initialization of weights to speed up the training process. The `weights_unit_rule` below is an helper function, it takes each layer as input `m`. By identify layer as Convolutioal or Linear Dense layer, we could assign different initializations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "smoking-pollution",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_rule(m):\n",
    "    classname = m.__class__.__name__\n",
    "    # Initialize convolutional weights as normal distribution (0, 0.01), bias as all ones\n",
    "    if classname.find('Conv') != -1:\n",
    "        # The no_grad flag is to prevent model from doing gradient calculation.\n",
    "        with torch.no_grad():\n",
    "            m.weight.normal_(0,0.04)\n",
    "            m.bias.fill_(1)\n",
    "    # Initialize Dense weights as uniform distribution of radius 1/sqrt n, n is the number of nodes\n",
    "    # in previous layer. The quantity 1/sqrt n is usually a good practice to speed up training. \n",
    "    if classname.find('Linear') != -1:\n",
    "        with torch.no_grad():\n",
    "            n = m.in_features\n",
    "            y = 1.0/np.sqrt(n)\n",
    "            m.weight.uniform_(-y,y)\n",
    "            m.bias.fill_(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "industrial-dragon",
   "metadata": {},
   "source": [
    "## Start Training\n",
    "\n",
    "Provide parameters such as number of classes, epochs and so on. After that, we can initialize the model and apply the weight initialization rule. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "electrical-breast",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [1, 16, 55, 55]           5,824\n",
      "       BatchNorm2d-2            [1, 16, 55, 55]              32\n",
      "            Conv2d-3            [1, 32, 55, 55]          12,832\n",
      "              ReLU-4            [1, 32, 55, 55]               0\n",
      "       BatchNorm2d-5            [1, 32, 55, 55]              64\n",
      "         MaxPool2d-6            [1, 32, 27, 27]               0\n",
      "            Conv2d-7            [1, 64, 27, 27]          18,496\n",
      "              ReLU-8            [1, 64, 27, 27]               0\n",
      "         MaxPool2d-9            [1, 64, 13, 13]               0\n",
      "           Conv2d-10           [1, 128, 13, 13]          73,856\n",
      "           Conv2d-11           [1, 128, 13, 13]         147,584\n",
      "             ReLU-12           [1, 128, 13, 13]               0\n",
      "        MaxPool2d-13             [1, 128, 6, 6]               0\n",
      "          Flatten-14                  [1, 4608]               0\n",
      "           Linear-15                  [1, 2048]       9,439,232\n",
      "             ReLU-16                  [1, 2048]               0\n",
      "          Dropout-17                  [1, 2048]               0\n",
      "           Linear-18                  [1, 1024]       2,098,176\n",
      "             ReLU-19                  [1, 1024]               0\n",
      "           Linear-20                     [1, 4]           4,100\n",
      "================================================================\n",
      "Total params: 11,800,196\n",
      "Trainable params: 11,800,196\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.59\n",
      "Forward/backward pass size (MB): 4.55\n",
      "Params size (MB): 45.01\n",
      "Estimated Total Size (MB): 50.16\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "num_classes = 4\n",
    "num_epochs = 10\n",
    "train_size = len(train_images)\n",
    "\n",
    "model = Alex_Custom().to(device)\n",
    "model.apply(weights_init_rule) \n",
    "\n",
    "# Using Stochastic Gradient Descent optimizer.\n",
    "# The Adam (Adaptive Momentum Estimator) has been tested, it is not better than SGD.\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "# torchsummary is used for print a summary about the model.\n",
    "torchsummary.summary(model, (3,227,227), 1, \"cpu\")\n",
    "# Cross Entropy is a good way to compute loss of classification problems.\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "brutal-abortion",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss every 20 batchs: 2.4, 2.21, 1.97, 1.74, 1.57, 1.44, 1.37, 1.33, 1.27, \n",
      "Epoch 1 Train Accuracy: 432 out of 787 = 54.89%. Time elapsed 45s.\n",
      "Test loss every 20 batchs: 0.36, 0.38, 0.44, 0.49, \n",
      "Epoch 1 Test Accuracy: 275 out of 338 = 81.36%.\n",
      "Training loss every 20 batchs: 0.86, 0.93, 0.99, 0.94, 0.91, 0.91, 0.88, 0.84, 0.83, \n",
      "Epoch 2 Train Accuracy: 520 out of 787 = 66.07%. Time elapsed 37s.\n",
      "Test loss every 20 batchs: 0.44, 0.47, 0.53, 0.55, \n",
      "Epoch 2 Test Accuracy: 272 out of 338 = 80.47%.\n",
      "Training loss every 20 batchs: 0.76, 0.8, 0.73, 0.78, 0.78, 0.78, 0.76, 0.8, 0.78, \n",
      "Epoch 3 Train Accuracy: 543 out of 787 = 69.0%. Time elapsed 38s.\n",
      "Test loss every 20 batchs: 0.53, 0.57, 0.6, 0.65, \n",
      "Epoch 3 Test Accuracy: 257 out of 338 = 76.04%.\n",
      "Training loss every 20 batchs: 0.86, 0.71, 0.76, 0.76, 0.73, 0.7, 0.72, 0.71, 0.7, \n",
      "Epoch 4 Train Accuracy: 555 out of 787 = 70.52%. Time elapsed 37s.\n",
      "Test loss every 20 batchs: 0.33, 0.36, 0.42, 0.48, \n",
      "Epoch 4 Test Accuracy: 283 out of 338 = 83.73%.\n",
      "Training loss every 20 batchs: 0.59, 0.68, 0.63, 0.61, 0.65, 0.67, 0.67, 0.67, 0.66, \n",
      "Epoch 5 Train Accuracy: 578 out of 787 = 73.44%. Time elapsed 37s.\n",
      "Test loss every 20 batchs: 0.29, 0.29, 0.37, 0.41, \n",
      "Epoch 5 Test Accuracy: 285 out of 338 = 84.32%.\n",
      "Training loss every 20 batchs: 0.68, 0.61, 0.66, 0.62, 0.63, 0.61, 0.6, 0.6, 0.62, \n",
      "Epoch 6 Train Accuracy: 595 out of 787 = 75.6%. Time elapsed 37s.\n",
      "Test loss every 20 batchs: 0.26, 0.31, 0.38, 0.42, \n",
      "Epoch 6 Test Accuracy: 287 out of 338 = 84.91%.\n",
      "Training loss every 20 batchs: 0.54, 0.56, 0.58, 0.54, 0.53, 0.53, 0.54, 0.55, 0.56, \n",
      "Epoch 7 Train Accuracy: 599 out of 787 = 76.11%. Time elapsed 37s.\n",
      "Test loss every 20 batchs: 0.32, 0.34, 0.41, 0.46, \n",
      "Epoch 7 Test Accuracy: 287 out of 338 = 84.91%.\n",
      "Training loss every 20 batchs: 0.5, 0.45, 0.45, 0.51, 0.54, 0.54, 0.54, 0.53, 0.52, \n",
      "Epoch 8 Train Accuracy: 607 out of 787 = 77.13%. Time elapsed 37s.\n",
      "Test loss every 20 batchs: 0.19, 0.22, 0.31, 0.35, \n",
      "Epoch 8 Test Accuracy: 292 out of 338 = 86.39%.\n",
      "Training loss every 20 batchs: 0.3, 0.49, 0.59, 0.56, 0.54, 0.53, 0.53, 0.54, 0.53, \n",
      "Epoch 9 Train Accuracy: 628 out of 787 = 79.8%. Time elapsed 37s.\n",
      "Test loss every 20 batchs: 0.52, 0.51, 0.55, 0.53, \n",
      "Epoch 9 Test Accuracy: 274 out of 338 = 81.07%.\n",
      "Training loss every 20 batchs: 0.44, 0.51, 0.57, 0.51, 0.48, 0.46, 0.45, 0.46, 0.44, \n",
      "Epoch 10 Train Accuracy: 640 out of 787 = 81.32%. Time elapsed 37s.\n",
      "Test loss every 20 batchs: 0.27, 0.33, 0.43, 0.47, \n",
      "Epoch 10 Test Accuracy: 280 out of 338 = 82.84%.\n",
      "Saving model...\n",
      "Saved!\n"
     ]
    }
   ],
   "source": [
    "train_accs, test_accs = [], []\n",
    "train_losses, test_losses = [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training mode\n",
    "    model.train()\n",
    "    accu_loss = 0\n",
    "    taccu_loss = 0\n",
    "    epoch_start = time.time()\n",
    "    steps = 20\n",
    "    accuracy_count = 0\n",
    "    print(f\"Training loss every {steps} batchs:\", end = \" \")\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        optimizer.zero_grad()\n",
    "        batch_images = data['image'].to(device)\n",
    "        batch_labels = data['label'].type(torch.LongTensor).to(device)\n",
    "        outputs = model(batch_images)\n",
    "        outputs_labels = [one_hot.index(max(one_hot)) for one_hot in outputs.tolist()]\n",
    "        accuracy_count += [batch_labels[i] == outputs_labels[i] for i in range(len(batch_labels))].count(True)\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "        accu_loss+=loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i%steps==steps-1:\n",
    "            train_loss = accu_loss/i\n",
    "            print(round(train_loss,2), end=\", \")\n",
    "    train_losses.append(train_loss)\n",
    "    print()\n",
    "    epoch_end = time.time()\n",
    "    epoch_duration = convert_sec(epoch_end-epoch_start)\n",
    "    train_acc  = 100*accuracy_count/train_n\n",
    "    train_accs.append(train_acc)\n",
    "    print(f\"Epoch {epoch+1} Train Accuracy: {accuracy_count} out of {train_n} = {round(train_acc, 2)}%. Time elapsed {epoch_duration}.\")\n",
    "    \n",
    "    # Evaluation mode\n",
    "    # evaluation on test set\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        t_count = 0\n",
    "        print(f\"Test loss every {steps} batchs:\", end = \" \")\n",
    "        for j, data in enumerate(testloader, 0):\n",
    "            tbatch_images = data['image'].to(device)\n",
    "            tbatch_labels = data['label'].type(torch.LongTensor).to(device)\n",
    "            toutputs = model(tbatch_images)\n",
    "            toutputs_labels = [one_hot.index(max(one_hot)) for one_hot in toutputs.tolist()]\n",
    "            t_count += [tbatch_labels[k] == toutputs_labels[k] for k in range(len(tbatch_labels))].count(True)\n",
    "            loss = criterion(toutputs, tbatch_labels)\n",
    "            taccu_loss += loss.item()\n",
    "            if j%steps==steps-1:\n",
    "                test_loss = taccu_loss/j\n",
    "                print(round(test_loss,2), end=\", \")\n",
    "        test_losses.append(test_loss)\n",
    "        print()\n",
    "        test_acc = 100*t_count/test_n\n",
    "        test_accs.append(test_acc)\n",
    "        print(f\"Epoch {epoch+1} Test Accuracy: {t_count} out of {test_n} = {round(test_acc,2)}%.\")\n",
    "    \n",
    "\"\"\"\n",
    "The code below save the model.\n",
    "It is enough to just save model.state_dict() for inferencing, \n",
    "but to save the model halfway and resume training in the future, \n",
    "we should also save the optimizer state_dict. \n",
    "\"\"\"\n",
    "print(\"Saving model...\")\n",
    "torch.save({\n",
    "        'epoch': num_epochs,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss\n",
    "    }, os.path.join(\"./Tutorial/\", f'model_Epoch{num_epochs}_{datetime.now()}.pth') )\n",
    "print(\"Saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sapphire-optimum",
   "metadata": {},
   "source": [
    "## Testing model\n",
    "\n",
    "We can grab some images from the web to check the performance of our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cathedral-hepatitis",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Output: [0.9064050912857056, -3.110182285308838, 3.942471742630005, 1.7003302574157715].\n",
      "Predicted class is shine.\n"
     ]
    }
   ],
   "source": [
    "# Evaluation mode\n",
    "model.eval()\n",
    "\n",
    "img = Image.open(urlopen(\"https://i.redd.it/i88wwl95in1z.jpg\"))\n",
    "img.show()   # Uncomment this to see the picture\n",
    "t = transform(img)\n",
    "# Note that the output array will not be the same, since the transform contains RandomCrop.\n",
    "y = model(t.unsqueeze(0)).tolist()[0]\n",
    "print(f\"Model Output: {y}.\")\n",
    "max_index = y.index(max(y))\n",
    "index_to_label = {0: 'cloudy', 1:'rain', 2:'shine', 3:q'sunrise'}\n",
    "print(f\"Predicted class is {index_to_label[max_index]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varying-investigation",
   "metadata": {},
   "source": [
    "## Resume Training\n",
    "\n",
    "To learn how to resume training, we reformulize our code and pack them into a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cognitive-benjamin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "\n",
    "num_classes = 4\n",
    "num_epochs = 10\n",
    "train_size = len(train_images)\n",
    "\n",
    "# A function block for resume training (Can also train a new one)    \n",
    "def train_and_save(num_classes, num_epochs, trainloader, testloader, train_n, test_n, pretrained, device):\n",
    "    # Detect any pretrained model. If there is any, resume training.\n",
    "    if pretrained:\n",
    "        # We still need to initialize model and optimizer, then attach loaded state_dicts to them\n",
    "        model = Alex_Custom().to(device)\n",
    "        optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "        try:\n",
    "            checkpoint = torch.load(pretrained)\n",
    "        except:\n",
    "            print(\"LoadModelError: Cannot load the model.\")\n",
    "            return\n",
    "        else:\n",
    "            print(\"This model is a trained model.\\nContinue training...\")\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            prev_epochs = checkpoint['epoch']\n",
    "            loss = checkpoint['loss']\n",
    "            print(f\"Resume training after epoch {prev_epochs}...\")\n",
    "    else:\n",
    "        # If not detected any model, start a new training\n",
    "        # So this block can replace the previous one.\n",
    "        print(\"This is a new model, start training...\")\n",
    "        model = Alex_Custom().to(device)\n",
    "        # Initialize weights only when our model is brand new\n",
    "        model.apply(weights_init_rule) \n",
    "        optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "        prev_epochs=0\n",
    "        \n",
    "    \n",
    "    torchsummary.summary(model, (3,227,227), 1, \"cuda\")\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    # The training is basically the same\n",
    "    # Only difference is we have to add up prev_epochs to show correct messages\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training mode\n",
    "        model.train()\n",
    "        accu_loss = 0\n",
    "        epoch_start = time.time()\n",
    "        steps = 20\n",
    "        accuracy_count = 0\n",
    "        print(f\"Training loss every {steps} batchs:\", end = \" \")\n",
    "        loader_n = len(trainloader)\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            optimizer.zero_grad()\n",
    "            batch_images = data['image'].to(device)\n",
    "            batch_labels = data['label'].type(torch.LongTensor).to(device)\n",
    "            outputs = model(batch_images)\n",
    "            outputs_labels = [one_hot.index(max(one_hot)) for one_hot in outputs.tolist()]\n",
    "            accuracy_count += [batch_labels[i] == outputs_labels[i] for i in range(len(batch_labels))].count(True)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            accu_loss+=loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if i%steps==steps-1:\n",
    "                print(round(accu_loss/i,2), end=\", \")\n",
    "        print()\n",
    "        epoch_end = time.time()\n",
    "        epoch_duration = convert_sec(epoch_end-epoch_start)\n",
    "        print(f\"Epoch {epoch+1+prev_epochs} Train Accuracy: {accuracy_count} out of {train_n} = {round(100*accuracy_count/train_n, 2)}%. Time elapsed {epoch_duration}.\")\n",
    "\n",
    "        # Evaluation mode\n",
    "        # evaluation on test set\n",
    "        model.eval()\n",
    "        t_count = 0\n",
    "        for j, data in enumerate(testloader, 0):\n",
    "            tbatch_images = data['image'].to(device)\n",
    "            tbatch_labels = data['label'].type(torch.LongTensor).to(device)\n",
    "            toutputs = model(tbatch_images)\n",
    "            toutputs_labels = [one_hot.index(max(one_hot)) for one_hot in toutputs.tolist()]\n",
    "            t_count += [tbatch_labels[i] == toutputs_labels[i] for i in range(len(tbatch_labels))].count(True)\n",
    "        print(f\"Epoch {epoch+1+prev_epochs} Test Accuracy: {t_count} out of {test_n} = {round(100*t_count/test_n,2)}%.\")\n",
    "        \n",
    "    \n",
    "    torch.save({\n",
    "        'epoch': num_epochs+prev_epochs,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss\n",
    "    }, os.path.join('./Tutorial/', f'model_Epoch{num_epochs+prev_epochs}_{datetime.now()}.pth') )\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sitting-fellow",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Replace the path below by your model.\n",
    "pretrained = './Tutorial/model_Epoch10_2021-03-24 09:49:42.409798.pth'\n",
    "train_and_save(num_classes, num_epochs, trainloader, testloader, train_n, test_n, pretrained, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
